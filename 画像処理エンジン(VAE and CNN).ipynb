{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yutaka0321/private/blob/main/%E7%94%BB%E5%83%8F%E5%87%A6%E7%90%86%E3%82%A8%E3%83%B3%E3%82%B8%E3%83%B3(VAE%20and%20CNN).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AyaJT0Eib4W"
      },
      "source": [
        "## VAE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "l55TusiRWQAo",
        "outputId": "20c030f5-a463-47ea-be02-b7284b1f9d8b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 1077.8826904296875\n",
            "Epoch 2, Loss: 1077.1873779296875\n",
            "Epoch 3, Loss: 1077.270751953125\n",
            "Epoch 4, Loss: 1077.2003173828125\n",
            "Epoch 5, Loss: 1076.9918212890625\n",
            "Epoch 6, Loss: 1077.1202392578125\n",
            "Epoch 7, Loss: 1077.0576171875\n",
            "Epoch 8, Loss: 1077.052001953125\n",
            "Epoch 9, Loss: 1076.8382568359375\n",
            "Epoch 10, Loss: 1077.0777587890625\n",
            "Epoch 11, Loss: 1076.9215087890625\n",
            "Epoch 12, Loss: 1076.9859619140625\n",
            "Epoch 13, Loss: 1076.7645263671875\n",
            "Epoch 14, Loss: 1076.787353515625\n",
            "Epoch 15, Loss: 1077.0855712890625\n",
            "Epoch 16, Loss: 1076.782470703125\n",
            "Epoch 17, Loss: 1076.8358154296875\n",
            "Epoch 18, Loss: 1076.855224609375\n",
            "Epoch 19, Loss: 1076.8773193359375\n",
            "Epoch 20, Loss: 1076.9703369140625\n",
            "Model weights saved.\n",
            "Evaluation Loss: 0.02157130432128906\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Flatten, Reshape, Conv2D, Conv2DTranspose, BatchNormalization, LeakyReLU\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.losses import binary_crossentropy\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "# Define the VAE model\n",
        "class VAE(tf.keras.Model):\n",
        "    def __init__(self, latent_dim):\n",
        "        super(VAE, self).__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "        # Encoder with BatchNormalization and LeakyReLU\n",
        "        self.encoder = tf.keras.Sequential([\n",
        "            Input(shape=(64, 64, 3)),\n",
        "            Conv2D(32, 3, strides=2, padding='same'),\n",
        "            BatchNormalization(),\n",
        "            LeakyReLU(alpha=0.2),\n",
        "            Conv2D(64, 3, strides=2, padding='same'),\n",
        "            BatchNormalization(),\n",
        "            LeakyReLU(alpha=0.2),\n",
        "            Flatten(),\n",
        "            Dense(latent_dim + latent_dim)  # Mean and log variance\n",
        "        ])\n",
        "\n",
        "        # Decoder with BatchNormalization and LeakyReLU\n",
        "        self.decoder = tf.keras.Sequential([\n",
        "            Input(shape=(latent_dim,)),\n",
        "            Dense(16 * 16 * 64, activation='relu'),\n",
        "            Reshape((16, 16, 64)),\n",
        "            Conv2DTranspose(64, 3, strides=2, padding='same'),\n",
        "            BatchNormalization(),\n",
        "            LeakyReLU(alpha=0.2),\n",
        "            Conv2DTranspose(32, 3, strides=2, padding='same'),\n",
        "            BatchNormalization(),\n",
        "            LeakyReLU(alpha=0.2),\n",
        "            Conv2DTranspose(3, 3, activation='sigmoid', padding='same')\n",
        "        ])\n",
        "\n",
        "    def call(self, inputs):\n",
        "        mean, log_var = tf.split(self.encoder(inputs), num_or_size_splits=2, axis=1)\n",
        "        z = self.reparameterize(mean, log_var)\n",
        "        reconstruction = self.decoder(z)\n",
        "        return reconstruction, mean, log_var\n",
        "\n",
        "    def reparameterize(self, mean, log_var):\n",
        "        epsilon = tf.random.normal(shape=mean.shape)\n",
        "        return mean + tf.exp(0.5 * log_var) * epsilon\n",
        "\n",
        "    def compute_loss(self, x, reconstruction, mean, log_var):\n",
        "        reconstruction_loss = binary_crossentropy(K.flatten(x), K.flatten(reconstruction))\n",
        "        kl_loss = -0.5 * tf.reduce_mean(log_var - tf.square(mean) - tf.exp(log_var) + 1)\n",
        "        return tf.reduce_mean(reconstruction_loss + kl_loss)\n",
        "\n",
        "# Prepare data with enhanced preprocessing\n",
        "def preprocess_image(image):\n",
        "    image = tf.image.resize(image, (64, 64))\n",
        "    image = image / 255.0  # Normalize to [0, 1] range\n",
        "    return image\n",
        "\n",
        "def load_data():\n",
        "    (x_train, _), (x_test, _) = tf.keras.datasets.cifar10.load_data()\n",
        "    x_train = np.array([preprocess_image(img) for img in x_train])\n",
        "    x_test = np.array([preprocess_image(img) for img in x_test])\n",
        "    return x_train, x_test\n",
        "\n",
        "# Train the VAE\n",
        "def train_vae():\n",
        "    x_train, x_test = load_data()\n",
        "    latent_dim = 16  # Increased latent dimension for better representation\n",
        "    vae = VAE(latent_dim)\n",
        "\n",
        "    # Reduce the learning rate for better convergence\n",
        "    vae.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005))\n",
        "\n",
        "    # Custom training loop\n",
        "    @tf.function\n",
        "    def train_step(x):\n",
        "        with tf.GradientTape() as tape:\n",
        "            reconstruction, mean, log_var = vae(x, training=True)\n",
        "            loss = vae.compute_loss(x, reconstruction, mean, log_var)\n",
        "        gradients = tape.gradient(loss, vae.trainable_variables)\n",
        "        vae.optimizer.apply_gradients(zip(gradients, vae.trainable_variables))\n",
        "        return loss\n",
        "\n",
        "    for epoch in range(20):  # Increased epochs for more training\n",
        "        epoch_loss = 0\n",
        "        for i in range(0, len(x_train), 32):\n",
        "            batch = x_train[i:i+32]\n",
        "            loss = train_step(batch)\n",
        "            epoch_loss += loss\n",
        "        print(f\"Epoch {epoch+1}, Loss: {epoch_loss.numpy()}\")\n",
        "\n",
        "    # Save the model weights after training\n",
        "    vae.save_weights(\"vae_weights_updated.weights.h5\")\n",
        "    print(\"Model weights saved.\")\n",
        "\n",
        "    # Evaluation on test data\n",
        "    test_loss = 0\n",
        "    for i in range(0, len(x_test), 32):\n",
        "        batch = x_test[i:i+32]\n",
        "        reconstruction, mean, log_var = vae(batch, training=False)\n",
        "        loss = vae.compute_loss(batch, reconstruction, mean, log_var)\n",
        "        test_loss += loss\n",
        "    print(f\"Evaluation Loss: {test_loss.numpy() / len(x_test)}\")\n",
        "\n",
        "# Run training\n",
        "train_vae()\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3r2pz74iU7P"
      },
      "source": [
        "## CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NHn_yITYFGN",
        "outputId": "46df452b-0320-4612-ff36-547f03cffb37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.8698 - loss: 0.4270\n",
            "Epoch 2/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9835 - loss: 0.0528\n",
            "Epoch 3/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9893 - loss: 0.0366\n",
            "Epoch 4/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9909 - loss: 0.0288\n",
            "Epoch 5/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9926 - loss: 0.0215\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "モデルが 'mnist_cnn_model.h5' として保存されました\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9883 - loss: 0.0366\n",
            "テスト精度: 0.9908000230789185\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# MNISTデータセットをロード\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "# データをCNNに適合する形にリシェイプし、正規化\n",
        "train_images = train_images.reshape((60000, 28, 28, 1)).astype('float32') / 255\n",
        "test_images = test_images.reshape((10000, 28, 28, 1)).astype('float32') / 255\n",
        "\n",
        "# ラベルをワンホットエンコーディング\n",
        "train_labels = to_categorical(train_labels)\n",
        "test_labels = to_categorical(test_labels)\n",
        "\n",
        "# CNNモデルを構築\n",
        "model = models.Sequential()\n",
        "\n",
        "# 畳み込み層\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "\n",
        "# 全結合層\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "# モデルのコンパイル\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# モデルの訓練\n",
        "model.fit(train_images, train_labels, epochs=5, batch_size=64)\n",
        "\n",
        "# モデルの保存\n",
        "model.save('mnist_cnn_model.h5')\n",
        "print(\"モデルが 'mnist_cnn_model.h5' として保存されました\")\n",
        "\n",
        "# モデルの評価\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(f\"テスト精度: {test_acc}\")\n",
        "\n",
        "# モデルの読み込み（必要に応じて使用）\n",
        "# loaded_model = tf.keras.models.load_model('mnist_cnn_model.h5')\n",
        "# loaded_model.evaluate(test_images, test_labels)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyMG9d+qxp2UvLV3+wjlj6Ym",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}